{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "os_sep = os.path.abspath(os.sep)\n",
    "wd = os.getcwd()\n",
    "dfs = os.path.join(os_sep, wd, 'opensmile', 'egemaps_summary_turns_zero_filtered') #the feature dfs of the interviews\n",
    "dem_dir = os.path.join(os_sep, 'dem_dir') #where to find the txt files with the group information of each participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "pauses = 'MeanUnvoicedSegmentLength'\n",
    "syll_rate = 'VoicedSegmentsPerSec'\n",
    "pitch = 'F0semitoneFrom27.5Hz_sma3nz_amean'\n",
    "loudness = 'loudness_sma3_amean'\n",
    "pitch_var = 'F0semitoneFrom27.5Hz_sma3nz_stddevNorm'\n",
    "\n",
    "features = [pauses, syll_rate, pitch, loudness, pitch_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-paris",
   "metadata": {},
   "source": [
    "#### Split each interview in conversation halves\n",
    "\n",
    "For each of the loaded dataframes, split the conversation in first and second half, separately for each speaker since we correlate the first and second part individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1_first_half = []\n",
    "ch1_second_half = []\n",
    "\n",
    "ch2_first_half = []\n",
    "ch2_second_half = []\n",
    "\n",
    "for file in sorted(glob.glob(dfs + '/*.csv')):\n",
    "    \n",
    "    df = pd.read_csv(file, sep = ';', index_col= [0])\n",
    "    \n",
    "    first_half, second_half = np.array_split(df.index, 2)\n",
    "    \n",
    "    if 'ch1' in file:\n",
    "        ch1_first_half.append(df.loc[first_half])\n",
    "        ch1_second_half.append(df.loc[second_half])\n",
    "        \n",
    "    else:\n",
    "        ch2_first_half.append(df.loc[first_half])\n",
    "        ch2_second_half.append(df.loc[second_half])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSynchronyFromDF(ch1_dfs, ch2_dfs, features):\n",
    "\n",
    "    import pandas as pd\n",
    "    import scipy.stats as stats\n",
    "    \n",
    "    #ToDo: fix later with loop\n",
    "    feature_rows = {'MeanUnvoicedSegmentLength' : [],\n",
    "                     'VoicedSegmentsPerSec' : [],\n",
    "                     'F0semitoneFrom27.5Hz_sma3nz_amean' : [],\n",
    "                     'loudness_sma3_amean' : [],\n",
    "                     'F0semitoneFrom27.5Hz_sma3nz_stddevNorm' : []}\n",
    "    \n",
    "    for ch1, ch2 in zip(ch1_dfs, ch2_dfs):\n",
    "        \n",
    "        sub_id = ch1['sub_id'].unique()[0]\n",
    "\n",
    "        for feature in features:\n",
    "\n",
    "            speaker_1 = ch1[feature].to_numpy()\n",
    "            speaker_2 = ch2[feature].to_numpy()\n",
    "            \n",
    "            #sometimes turns will be unequal, in that case drop the last one from the array\n",
    "            if len(speaker_1) > len(speaker_2):\n",
    "                speaker_1 = speaker_1[:-1]\n",
    "                \n",
    "            elif len(speaker_1) < len(speaker_2):\n",
    "                speaker_2 = speaker_2[:-1]\n",
    "                \n",
    "            speaker_1 = speaker_1[~np.isnan(speaker_2)]  #drop nan turns from ch2 also from ch1  \n",
    "            speaker_2 = speaker_2[~np.isnan(speaker_2)]\n",
    "    \n",
    "            x = speaker_1[~np.isnan(speaker_1)] #drop nan turns from ch1 also from ch2  \n",
    "            y = speaker_2[~np.isnan(speaker_1)]\n",
    "            \n",
    "            #calculate synchrony using spearman r\n",
    "            r, p = stats.spearmanr(x, y)\n",
    "            \n",
    "            #transform to z scores\n",
    "            r_z = np.arctanh(r)\n",
    "            \n",
    "            #create dictionary with all the information\n",
    "            row = {'soundname': sub_id,\n",
    "                   'r': r, \n",
    "                   'p': p, \n",
    "                   'r_z': r_z}\n",
    "        \n",
    "            feature_rows[feature] += [row]\n",
    "\n",
    "        \n",
    "    return feature_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-congo",
   "metadata": {},
   "source": [
    "#### Calculate speech accommodation for the first and the second halves of the interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_rows_first_half = calculateSynchronyFromDF(ch1_first_half, ch2_first_half, features)\n",
    "feature_rows_second_half = calculateSynchronyFromDF(ch1_second_half, ch2_second_half, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dfs_first_half = {}\n",
    "summary_dfs_second_half = {}\n",
    "\n",
    "for feature, rows in feature_rows_first_half.items():\n",
    "    \n",
    "    summary_dfs_first_half[feature] =  pd.DataFrame(rows)\n",
    "    \n",
    "for feature, rows in feature_rows_second_half.items():\n",
    "    \n",
    "    summary_dfs_second_half[feature] =  pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-travel",
   "metadata": {},
   "source": [
    "#### Load the group splits and compare halves of healthy controls and SZ patients separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = np.loadtxt(os.path.join(dem_dir, 'control_subs.txt'), dtype= str)\n",
    "patients = np.loadtxt(os.path.join(dem_dir, 'patient_subs.txt'), dtype= str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroupIndices(df, group):\n",
    "    \n",
    "    group_indices = [k for k in df['soundname'] if k[:4] in group]\n",
    "    \n",
    "    return group_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairedTestPerFeature(features, dfs_condition1, dfs_condition2, group):\n",
    "    \n",
    "    import scipy.stats as stats\n",
    "\n",
    "    rows = {}\n",
    "    \n",
    "    for feature in features:\n",
    "\n",
    "        row = {}\n",
    "\n",
    "        cond1 = dfs_condition1[feature]\n",
    "        cond2 = dfs_condition2[feature]\n",
    "\n",
    "        idxs_group = getGroupIndices(cond1, group) #the matching group subjects in the dataframe\n",
    "\n",
    "        x = cond1[cond1['soundname'].isin(idxs_group)]['r_z']   #select converted r value\n",
    "        y = cond2[cond2['soundname'].isin(idxs_group)]['r_z']  \n",
    "\n",
    "        #paired ttest!\n",
    "        t, p = stats.ttest_rel(x, y)\n",
    "\n",
    "        row['T'] = t\n",
    "        row['p'] = p\n",
    "\n",
    "        rows[feature] = row\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df_controls = pairedTestPerFeature(features, summary_dfs_first_half, summary_dfs_second_half, controls)\n",
    "t_df_patients = pairedTestPerFeature(features, summary_dfs_first_half, summary_dfs_second_half, patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-cycle",
   "metadata": {},
   "source": [
    "#### Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df_controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df_patients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-ability",
   "metadata": {},
   "source": [
    "#### Repeat same process with conversation thirds instead of halves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1_first = []\n",
    "ch1_second = []\n",
    "ch1_third = []\n",
    "\n",
    "ch2_first = []\n",
    "ch2_second = []\n",
    "ch2_third = []\n",
    "\n",
    "\n",
    "for file in sorted(glob.glob(dfs + '/*.csv')):\n",
    "    \n",
    "    df = pd.read_csv(file, sep = ';', index_col= [0])\n",
    "    \n",
    "    first, second, third  = np.array_split(df.index, 3)\n",
    "    \n",
    "    if 'ch1' in file:\n",
    "        ch1_first.append(df.loc[first])\n",
    "        ch1_second.append(df.loc[second])\n",
    "        ch1_third.append(df.loc[third])\n",
    "        \n",
    "    else:\n",
    "        ch2_first.append(df.loc[first])\n",
    "        ch2_second.append(df.loc[second])\n",
    "        ch2_third.append(df.loc[third])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-inquiry",
   "metadata": {},
   "source": [
    "#### Calculate speech accommodation for each third of the interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_rows_first = calculateSynchronyFromDF(ch1_first, ch2_first, features)\n",
    "feature_rows_second = calculateSynchronyFromDF(ch1_second, ch2_second, features)\n",
    "feature_rows_third = calculateSynchronyFromDF(ch1_third, ch2_third, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDFsFromDict(feature_dict):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    summary_dfs = {}\n",
    "    \n",
    "    for feature, rows in feature_dict.items():\n",
    "    \n",
    "        summary_dfs[feature] =  pd.DataFrame(rows)\n",
    "    \n",
    "    return summary_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dfs_first = makeDFsFromDict(feature_rows_first)\n",
    "summary_dfs_second = makeDFsFromDict(feature_rows_second)\n",
    "summary_dfs_third = makeDFsFromDict(feature_rows_third)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-cinema",
   "metadata": {},
   "source": [
    "#### Perform t-tests between the first and second and the second and third conversation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_vs_second_cntrl = pairedTestPerFeature(features, summary_dfs_first, summary_dfs_second, controls)\n",
    "second_vs_third_cntrl = pairedTestPerFeature(features, summary_dfs_second, summary_dfs_third, controls)\n",
    "\n",
    "\n",
    "first_vs_second_sz = pairedTestPerFeature(features, summary_dfs_first, summary_dfs_second, patients)\n",
    "second_vs_third_sz = pairedTestPerFeature(features, summary_dfs_second, summary_dfs_third, patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-uruguay",
   "metadata": {},
   "source": [
    "#### print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_vs_second_cntrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_vs_third_cntrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_vs_second_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_vs_third_sz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-uzbekistan",
   "metadata": {},
   "source": [
    "#### Plot the results\n",
    "\n",
    "To make use of seaborn's high level integration of dataframes we reshape the data a bit and take the mean for each conversation part, so that all values from all channels, features and time points are in one dataframe. For that we merge the individual dfs for each interview and add a few columns with extra information for conversation halves and thirds respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1_first_mean = []\n",
    "ch2_first_mean = []\n",
    "\n",
    "ch1_second_mean = []\n",
    "ch2_second_mean = []\n",
    "\n",
    "#for all interviews, take the mean\n",
    "for ch1, ch2 in zip(ch1_first_half, ch2_first_half):\n",
    "    \n",
    "    sub_id = pd.Series(data = ch1['sub_id'].unique()[0])\n",
    "                       \n",
    "    ch1_first_mean.append(ch1.mean().append(sub_id))\n",
    "    ch2_first_mean.append(ch2.mean().append(sub_id))\n",
    "    \n",
    "for ch1, ch2 in zip(ch1_second_half, ch2_second_half):\n",
    "                       \n",
    "    sub_id = pd.Series(data = ch1['sub_id'].unique()[0])\n",
    "                   \n",
    "    ch1_second_mean.append(ch1.mean().append(sub_id))\n",
    "    ch2_second_mean.append(ch2.mean().append(sub_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge all interviews in one dataframe\n",
    "ch1_first_df = pd.DataFrame(ch1_first_mean)\n",
    "ch2_first_df = pd.DataFrame(ch2_first_mean)\n",
    "\n",
    "ch1_second_df = pd.DataFrame(ch1_second_mean)\n",
    "ch2_second_df = pd.DataFrame(ch2_second_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the first and second half, add information which part each conversation belongs to\n",
    "ch1_first_df['time'] = '1/2'\n",
    "ch2_first_df['time'] = '1/2'\n",
    "\n",
    "ch1_second_df['time'] = '2/2'\n",
    "ch2_second_df['time'] = '2/2'\n",
    "\n",
    "ch1 = pd.concat([ch1_first_df, ch1_second_df])\n",
    "ch2 = pd.concat([ch2_first_df, ch2_second_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1['channel'] = 'Interviewer'\n",
    "ch2['channel'] = 'Participant'\n",
    "\n",
    "conversation_halves = pd.concat([ch1, ch2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_halves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1_first_mean = []\n",
    "ch1_second_mean = []\n",
    "ch1_third_mean = []\n",
    "\n",
    "ch2_first_mean = []\n",
    "ch2_second_mean = []\n",
    "ch2_third_mean = []\n",
    "\n",
    "#for each interview, take the mean of the first, second and third part\n",
    "for ch1, ch2 in zip(ch1_first, ch2_first):\n",
    "    \n",
    "    sub_id = pd.Series(data = ch1['sub_id'].unique()[0])\n",
    "    \n",
    "    ch1_first_mean.append(ch1.mean().append(sub_id))\n",
    "    ch2_first_mean.append(ch2.mean().append(sub_id))\n",
    "    \n",
    "for ch1, ch2 in zip(ch1_second, ch2_second):\n",
    "    \n",
    "    sub_id = pd.Series(data = ch1['sub_id'].unique()[0])\n",
    "    \n",
    "    ch1_second_mean.append(ch1.mean().append(sub_id))\n",
    "    ch2_second_mean.append(ch2.mean().append(sub_id))\n",
    "\n",
    "for ch1, ch2 in zip(ch1_third, ch2_third):\n",
    "    \n",
    "    sub_id = pd.Series(data = ch1['sub_id'].unique()[0])\n",
    "\n",
    "    ch1_third_mean.append(ch1.mean().append(sub_id))\n",
    "    ch2_third_mean.append(ch2.mean().append(sub_id))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge all interviews into a dataframe\n",
    "ch1_first_df = pd.DataFrame(ch1_first_mean)\n",
    "ch2_first_df = pd.DataFrame(ch2_first_mean)\n",
    "\n",
    "ch1_second_df = pd.DataFrame(ch1_second_mean)\n",
    "ch2_second_df = pd.DataFrame(ch2_second_mean)\n",
    "\n",
    "ch1_third_df = pd.DataFrame(ch1_third_mean)\n",
    "ch2_third_df = pd.DataFrame(ch2_third_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all parts, add information which third the row belongs to\n",
    "ch1_first_df['time'] = '1/3'\n",
    "ch2_first_df['time'] = '1/3'\n",
    "\n",
    "ch1_second_df['time'] = '2/3'\n",
    "ch2_second_df['time'] = '2/3'\n",
    "\n",
    "ch1_third_df['time'] = '3/3'\n",
    "ch2_third_df['time'] = '3/3'\n",
    "\n",
    "ch1 = pd.concat([ch1_first_df, ch1_second_df, ch1_third_df])\n",
    "ch2 = pd.concat([ch2_first_df, ch2_second_df, ch2_third_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1['channel'] = 'Interviewer'\n",
    "ch2['channel'] = 'Participant'\n",
    "\n",
    "conversation_thirds = pd.concat([ch1, ch2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_halves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_thirds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the dataframes for controls and patients\n",
    "\n",
    "conversation_halves_controls = conversation_halves.loc[conversation_halves[0].isin(controls)]\n",
    "conversation_halves_patients = conversation_halves.loc[conversation_halves[0].isin(patients)]\n",
    "\n",
    "conversation_thirds_controls = conversation_thirds.loc[conversation_thirds[0].isin(controls)]\n",
    "conversation_thirds_patients = conversation_thirds.loc[conversation_thirds[0].isin(patients)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-validation",
   "metadata": {},
   "source": [
    "#### Make a plot that shows the mean values for all speech features across the different time splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-confusion",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=len(features), ncols=2, figsize = (10, 20))\n",
    "features_to_plot = sorted(features * 2)\n",
    "\n",
    "#one column contains halves, the other thirds all other labels stay the same\n",
    "y_labels = ['Pitch', 'Pitch', \n",
    "            'Pitch Variability', 'Pitch Variability', \n",
    "            'Average Pause Duration', 'Average Pause Duration',\n",
    "            'Syllable Rate', 'Syllable Rate',\n",
    "            'Loudness', 'Loudness']\n",
    "\n",
    "#defining these manually so the y axes for halves and thirds are the same\n",
    "y_lims = [(20, 35), (20, 35),\n",
    "          (0.1, 0.2), (0.1, 0.2),\n",
    "          (0, 0.6), (0, 0.6),\n",
    "          (1.5, 6), (1.5, 6),\n",
    "          (0.2, 0.8), (0.2, 0.8)]\n",
    "\n",
    "#custom legend showing speaker and group attribution\n",
    "legend_elements = [\n",
    "                   Line2D([0], [0], marker='o', label='Interviewer', markerfacecolor='lightgrey', markersize=10, color = 'lightgrey'),\n",
    "                   Line2D([0], [0], marker='x', label='Participant', markerfacecolor='grey', markersize=10, color = 'dimgrey'),\n",
    "                   Line2D([0], [0], label='Control Group', linestyle = '--'),\n",
    "                   Line2D([0], [0], label='Patient Group', color = 'red'),\n",
    "                   ]\n",
    "\n",
    "plt.suptitle('Average Speech Features Across the Interviews', fontsize = 15, y=1.0, x =0.45)\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    \n",
    "    #plot conversation halves on the left column\n",
    "    if (i % 2) == 0:\n",
    "        \n",
    "        #plot interviewer + control participant\n",
    "        sns.pointplot(x=\"time\", y=features_to_plot[i], hue=\"channel\",\n",
    "                     capsize=.2, height=6, aspect=.75,\n",
    "                     kind=\"point\", data=conversation_halves_controls, palette = \"Blues\", ax = ax,\n",
    "                    markers=[\"o\", \"x\"], linestyles=[\"--\", \"--\"])\n",
    "        \n",
    "        #plot interviewer + patient\n",
    "        sns.pointplot(x=\"time\", y=features_to_plot[i], hue=\"channel\",\n",
    "                     capsize=.2, height=6, aspect=.75,\n",
    "                     kind=\"point\", data=conversation_halves_patients, ax = ax, palette = 'Reds',\n",
    "                     markers = ['o', 'x'])\n",
    "        \n",
    "        ax.get_legend().remove() #one legend per row\n",
    "        ax.set_ylabel(y_labels[i], fontsize = 14)\n",
    "        ax.set_xlabel('Conversation Halves', fontsize = 14)\n",
    "        ax.set_ylim(y_lims[i])\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #plot interviewer + control participant\n",
    "        sns.pointplot(x=\"time\", y=features_to_plot[i], hue=\"channel\",\n",
    "                       capsize=.2, height=6, aspect=.75,\n",
    "                       kind=\"point\", data=conversation_thirds_controls, palette = \"Blues\", ax = ax,\n",
    "                       markers=[\"o\", \"x\"], linestyles=[\"--\", \"--\"])\n",
    "        \n",
    "        #plot interviewer + patient\n",
    "        sns.pointplot(x=\"time\", y=features_to_plot[i], hue=\"channel\",\n",
    "                       capsize=.2, height=6, aspect=.75,\n",
    "                       kind=\"point\", data=conversation_thirds_patients, ax = ax, palette = 'Reds',\n",
    "                       markers = ['o', 'x'])\n",
    "        \n",
    "        #add custom legend\n",
    "        ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5), fontsize = 13)\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xlabel('Conversation Thirds', fontsize = 14)\n",
    "        ax.set_ylim(y_lims[i])\n",
    "        \n",
    "        \n",
    "        \n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
