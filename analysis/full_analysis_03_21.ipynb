{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "- installed PRAAT\n",
    "- installed openSMILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change these paths:\n",
    "\n",
    "1. openSMILE installation\n",
    "2. path to the PRAAT application (win: \"C:\\Program Files\\Praat.exe\", mac: /Applications/Praat.app/Contents/MacOS/Praat, linux:  /usr/bin/praat)\n",
    "3. the directory of the audio files\n",
    "4. an (empty) data directory folder where you want to store anonymized files such as textgrids or dataframes with the audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_sep = os.path.abspath(os.sep)\n",
    "opensmile_dir = os.path.join(os_sep, '') #openSMILE installation location\n",
    "praat_path = os.path.join(os_sep, 'Applications','Praat.app', 'Contents', 'MacOS', 'Praat') # PRAAT execetuable\n",
    "audio_dir = os.path.join(os_sep, '') # audio data\n",
    "data_dir = os.path.join(os_sep, '') #csv output\n",
    "#txt files that contain the subject id's of the participants in that group\n",
    "dem_dir = os.path.join(os_sep, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDirs(dir_list):\n",
    "    \"\"\"Checks if the given directories in the list exist, if not directories are created.\"\"\"\n",
    "    import os\n",
    "    \n",
    "    for path in dir_list:\n",
    "        try:\n",
    "            os.stat(path)\n",
    "        except:\n",
    "            os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Split audio data with PRAAT\n",
    "\n",
    "Audio files contain two channels for the interviewer (ch1) and participant (ch2). We split those channels and save them separately to extract features from each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "#define and create directories\n",
    "split_ch_output = os.path.join(audio_dir, 'split_channels')\n",
    "temp_dir = os.path.join(audio_dir, 'tmp')\n",
    "\n",
    "checkDirs([split_ch_output, temp_dir])\n",
    "\n",
    "\n",
    "#praat script to split files\n",
    "split_script = os.path.join(os.getcwd(), 'PRAATScripts', 'separate_channels.praat')\n",
    "\n",
    "#praat script doesn't handle many files easily, to prevent memory overflowing we process files in batches of 10 at a time\n",
    "file_list = glob.glob(audio_dir + os_sep + '*.wav')\n",
    "n_batches = int(len(file_list) / 10.)\n",
    "batches = np.array_split(file_list, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(batches):\n",
    "    \n",
    "    #move file back and forth between tmp and audio folder\n",
    "    for file in batch:\n",
    "        new_path = os.path.join(temp_dir, os.path.basename(file))\n",
    "        os.system(f'mv {file} {new_path}')\n",
    "    \n",
    "    #run praat script on batch with arguments\n",
    "    subprocess.call([praat_path, \n",
    "                '--run',\n",
    "                split_script,     #path to script\n",
    "                temp_dir + os_sep, #input dir + praat needs the slash at the end of a path\n",
    "                split_ch_output + os_sep]) #output dir\n",
    "    \n",
    "    for file in batch:\n",
    "        tmp_path = os.path.join(temp_dir, os.path.basename(file))\n",
    "        os.system(f'mv {tmp_path} {file}')\n",
    "    \n",
    "    print(f'finished batch {i + 1} out of {n_batches}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Concatenate and annotate the audio files with PRAAT\n",
    "\n",
    "To automatically annotate the turns of each speaker, a PRAAT script detects the silences in the interviewer track (ch1) and annotates these silences ina TextGrid file. If the interviewer is silent (i.e., between asking questions) it is assumed that the participant is speaking. The concatenated audio files are all speaking turns concatenated into one audio track, separately for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directories to save concatenated audio and generated textgrids\n",
    "concat_ch_output = os.path.join(audio_dir, 'concatenated_channels')\n",
    "textgrid_dir = os.path.join(data_dir, 'textgrids')\n",
    "turn_textgrids = os.path.join(data_dir, 'textgrids', 'turn_textgrids')\n",
    "\n",
    "checkDirs([concat_ch_output, textgrid_dir, turn_textgrids, temp_dir])\n",
    "\n",
    "#PRAAT script\n",
    "concat_script = os.path.join(os.getcwd(), 'PRAATScripts', 'praat_splitsen.praat')\n",
    "\n",
    "#again in batches of 10 to prevent PRAAT from crashing\n",
    "file_list = glob.glob(audio_dir + os_sep + '*.wav')\n",
    "n_batches = int(len(file_list) / 10.)\n",
    "batches = np.array_split(file_list, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(batches):\n",
    "    for file in batch:\n",
    "        new_path = os.path.join(temp_dir, os.path.basename(file))\n",
    "        os.system(f'mv {file} {new_path}')\n",
    "    \n",
    "    #run praat script on batch\n",
    "    subprocess.call([praat_path, \n",
    "                '--run',\n",
    "                concat_script,    #path to script\n",
    "                temp_dir + os_sep, #praat needs the slash at the end of a path\n",
    "                concat_ch_output + os_sep, #output audio\n",
    "                turn_textgrids + os_sep])  #output textgrids\n",
    "    \n",
    "    for file in batch:\n",
    "        tmp_path = os.path.join(temp_dir, os.path.basename(file))\n",
    "        os.system(f'mv {tmp_path} {file}')\n",
    "    \n",
    "    print(f'finished batch {i + 1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1. Feature extraction with eGeMAPS in openSMILE\n",
    "\n",
    "openSMILE has different configuration files and arguments to extract features. Most of the audio features (i.e., pitch or loudness) are computed every 10ms and then summarized (mean, std) over a given period of time. We extract the start and end time for each turn from the TextGrid files we just created and get the summarized features for each turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from praatio import tgio\n",
    "import subprocess\n",
    "\n",
    "#define and create input and output directories\n",
    "audio = os.path.join(audio_dir, 'split_channels')\n",
    "textgrid_dir = os.path.join(data_dir, 'textgrids', 'turn_textgrids')\n",
    "config_file = os.path.join(opensmile_dir, 'config', 'gemaps', 'eGeMAPSv01a.conf')\n",
    "\n",
    "egemaps_output = os.path.join(data_dir, 'opensmile', 'egemaps_summary_turns')\n",
    "\n",
    "checkDirs([egemaps_output])\n",
    "\n",
    "for file in sorted(glob.glob(audio + '/*.wav')):\n",
    "    \n",
    "    #load textgrid with turn annotations\n",
    "    sub_id = os.path.basename(file)[:4]\n",
    "    textgrid = sorted(glob.glob(os.path.join(textgrid_dir, sub_id + '*.TextGrid')))\n",
    "    \n",
    "    filename = os.path.basename(os.path.normpath(file))\n",
    "    output_file = os.path.join(egemaps_output, filename[:-4] + '.csv')\n",
    "    \n",
    "    #read textgrid using praatio, extract entries of the annotated tier\n",
    "    tg = tgio.openTextgrid(textgrid[0])\n",
    "    entryList = tg.tierDict['silences'].entryList\n",
    "\n",
    "    intervals_interviewer = []\n",
    "    intervals_participant = []\n",
    "    \n",
    "    #sort entries by speaker\n",
    "    for entry in entryList:\n",
    "        start = entry[0]\n",
    "        stop = entry[1]\n",
    "        \n",
    "        if entry[2] == 'interviewer_silent':\n",
    "            \n",
    "            intervals_participant.append((start, stop))\n",
    "            \n",
    "        if entry[2] == 'interviewer_speaks':\n",
    "            \n",
    "            intervals_interviewer.append((start, stop))\n",
    "    \n",
    "    #select correct file for speaker\n",
    "    if 'ch1' in file:\n",
    "        \n",
    "        for start, stop in intervals_interviewer:\n",
    "            \n",
    "            #name that's displayed in column of output file\n",
    "            instname = str(start) + '-' + str(stop)\n",
    "            \n",
    "            #run openSMILE extraction with arguments\n",
    "            subprocess.run(['SMILExtract', \n",
    "                            '-C', config_file,   #egemaps configuration\n",
    "                            '-I', file,          #audio file\n",
    "                            '-csvoutput', output_file,  #csv summary file\n",
    "                            '-start', str(start),    #time interval from which features are extracted\n",
    "                            '-end', str(stop),\n",
    "                            '-instname', instname])  #start and end for each turn\n",
    "    elif 'ch2' in file:\n",
    "        \n",
    "        for start, stop in intervals_participant:\n",
    "             \n",
    "            #name that's displayed in column of output file\n",
    "            instname = str(start) + '-' + str(stop)\n",
    "            \n",
    "            #run openSMILE extraction with arguments\n",
    "            subprocess.run(['SMILExtract', \n",
    "                            '-C', config_file, \n",
    "                            '-I', file, \n",
    "                            '-csvoutput', output_file,\n",
    "                            '-start', str(start),\n",
    "                            '-end', str(stop),\n",
    "                            '-instname', instname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out of all the egemaps features these are the ones of interest\n",
    "pauses = 'MeanUnvoicedSegmentLength'\n",
    "syll_rate = 'VoicedSegmentsPerSec'\n",
    "pitch = 'F0semitoneFrom27.5Hz_sma3nz_amean'\n",
    "loudness = 'loudness_sma3_amean'\n",
    "pitch_var = 'F0semitoneFrom27.5Hz_sma3nz_stddevNorm'\n",
    "\n",
    "features = [pauses, syll_rate, pitch, loudness, pitch_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2. Pre-process the data\n",
    "\n",
    "Since the turns of each speaker are annotated automatically, the process is error prone. Specifially, the speaking turns are annotated based on the interviewer track, anytime the interviewer is pausing, it is assumed the participant is speaking. That is obviously not always true, leading to trailing silences in the participant track and false positives, where an entire interval is falsely labeled as speech. To exclude such false positives, only sounding intervals (F0 > 0) are used for analysis. OpenSMILE already thresholds the pitch data, so all pitch values that are 0.0 are replaced by NaN values. At any point where pitch = 0, loudness, syllable rate and pitch variability are also set to 0 and replaced by NaN, as these can only be computed from speaking intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egemaps_output = os.path.join(data_dir, 'opensmile', 'egemaps_summary_turns')\n",
    "filtered_output = os.path.join(data_dir, 'opensmile', 'egemaps_summary_turns_zero_filtered')\n",
    "\n",
    "checkDirs([filtered_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1_files = []\n",
    "ch2_files = []\n",
    "\n",
    "for file in sorted(glob.glob(egemaps_output + '/*.csv')):\n",
    "    if 'ch1' in file:\n",
    "        ch1_files.append(file)\n",
    "    else:\n",
    "        ch2_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "for ch1, ch2 in zip(ch1_files, ch2_files):\n",
    "    \n",
    "    df_ch1 = pd.read_csv(ch1, sep = ';')\n",
    "    df_ch2 = pd.read_csv(ch2, sep = ';')\n",
    "    \n",
    "    cols_to_keep = features[:]\n",
    "    cols_to_keep.insert(0, 'name') #keep the name column without updating features\n",
    "    \n",
    "    df_ch1_filt = df_ch1[cols_to_keep]\n",
    "    df_ch2_filt = df_ch2[cols_to_keep]\n",
    "    \n",
    "    #set remaining columns to zero wherever pitch is zero\n",
    "    df_ch1_filt.loc[df_ch1_filt['F0semitoneFrom27.5Hz_sma3nz_amean'] == 0.0, \n",
    "               ['loudness_sma3_amean', 'VoicedSegmentsPerSec', 'F0semitoneFrom27.5Hz_sma3nz_stddevNorm']] = 0.0\n",
    "\n",
    "    df_ch2_filt.loc[df_ch2_filt['F0semitoneFrom27.5Hz_sma3nz_amean'] == 0.0, \n",
    "               ['loudness_sma3_amean', 'VoicedSegmentsPerSec', 'F0semitoneFrom27.5Hz_sma3nz_stddevNorm']] = 0.0\n",
    "    \n",
    "    #replace zeros with nans\n",
    "    df_ch1_filt[['F0semitoneFrom27.5Hz_sma3nz_amean', \n",
    "                               'loudness_sma3_amean', \n",
    "                               'VoicedSegmentsPerSec',\n",
    "                               'F0semitoneFrom27.5Hz_sma3nz_stddevNorm']].replace(0.0, np.nan)\n",
    "    df_ch2_filt[['F0semitoneFrom27.5Hz_sma3nz_amean', \n",
    "                               'loudness_sma3_amean', \n",
    "                               'VoicedSegmentsPerSec',\n",
    "                               'F0semitoneFrom27.5Hz_sma3nz_stddevNorm']].replace(0.0, np.nan)\n",
    "    \n",
    "    #keep track of sub_id for doubel checking\n",
    "    sub_id =  os.path.basename(ch1)[:4]\n",
    "    \n",
    "    df_ch1_filt['sub_id'] = sub_id\n",
    "    df_ch2_filt['sub_id'] = sub_id\n",
    "\n",
    "    \n",
    "    filename_ch1 = os.path.basename(ch1)[:-4]\n",
    "    filename_ch2 = os.path.basename(ch2)[:-4]\n",
    "    \n",
    "    df_ch1_filt.to_csv(os.path.join(filtered_output, filename_ch1 + '_zero_drop.csv'), sep = ';')\n",
    "    df_ch2_filt.to_csv(os.path.join(filtered_output, filename_ch2 + '_zero_drop.csv'), sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1_files_filt = []\n",
    "ch2_files_filt = []\n",
    "\n",
    "for file in sorted(glob.glob(filtered_output + '/*.csv')):\n",
    "    if 'ch1' in file:\n",
    "        ch1_files_filt.append(file)\n",
    "    else:\n",
    "        ch2_files_filt.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSynchrony(ch1_files, ch2_files, features):\n",
    "\n",
    "    import pandas as pd\n",
    "    import scipy.stats as stats\n",
    "    \n",
    "    #ToDo: fix later with loop\n",
    "    feature_rows = {'MeanUnvoicedSegmentLength' : [],\n",
    "                     'VoicedSegmentsPerSec' : [],\n",
    "                     'F0semitoneFrom27.5Hz_sma3nz_amean' : [],\n",
    "                     'loudness_sma3_amean' : [],\n",
    "                     'F0semitoneFrom27.5Hz_sma3nz_stddevNorm' : []}\n",
    "    \n",
    "    for ch1, ch2 in zip(ch1_files, ch2_files):\n",
    "        \n",
    "        ch1_df = pd.read_csv(ch1, sep = ';', index_col= [0])\n",
    "        ch2_df = pd.read_csv(ch2, sep = ';', index_col= [0])\n",
    "        \n",
    "        sub_id = os.path.basename(ch1)[:4]\n",
    "\n",
    "        for feature in features:\n",
    "\n",
    "            speaker_1 = ch1_df[feature].to_numpy()\n",
    "            speaker_2 = ch2_df[feature].to_numpy()\n",
    "            \n",
    "            #sometimes turns will be unequal, in that case drop the last one from the array\n",
    "            if len(speaker_1) > len(speaker_2):\n",
    "                speaker_1 = speaker_1[:-1]\n",
    "                \n",
    "            elif len(speaker_1) < len(speaker_2):\n",
    "                speaker_2 = speaker_2[:-1]\n",
    "                \n",
    "            speaker_1 = speaker_1[~np.isnan(speaker_2)]  #drop nan turns from ch2 also from ch1  \n",
    "            speaker_2 = speaker_2[~np.isnan(speaker_2)]\n",
    "    \n",
    "            x = speaker_1[~np.isnan(speaker_1)] #drop nan turns from ch1 also from ch2  \n",
    "            y = speaker_2[~np.isnan(speaker_1)]\n",
    "            \n",
    "            #calculate synchrony using spearman r\n",
    "            r, p = stats.spearmanr(x, y)\n",
    "            \n",
    "            #transform to z scores\n",
    "            r_z = np.arctanh(r)\n",
    "\n",
    "            row = {'soundname': sub_id,\n",
    "                   'r': r, \n",
    "                   'p': p, \n",
    "                   'r_z': r_z}\n",
    "        \n",
    "            feature_rows[feature] += [row]\n",
    "\n",
    "        \n",
    "    return feature_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_rows = calculateSynchrony(ch1_files_filt, ch2_files_filt, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dir = os.path.join(data_dir, 'group_level')\n",
    "\n",
    "checkDirs([summary_dir])\n",
    "\n",
    "for feature, rows in feature_rows.items():\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    df.to_csv(os.path.join(summary_dir, feature + '_summary.csv'), sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4. make group comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dir = os.path.join(data_dir, 'group_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load summary dataframes\n",
    "pitch = pd.read_csv(os.path.join(summary_dir, 'F0semitoneFrom27.5Hz_sma3nz_amean_summary.csv'), sep = ';', index_col = [0])\n",
    "loudness = pd.read_csv(os.path.join(summary_dir, 'loudness_sma3_amean_summary.csv'), sep = ';', index_col = [0])\n",
    "syll = pd.read_csv(os.path.join(summary_dir, 'VoicedSegmentsPerSec_summary.csv'), sep = ';', index_col = [0])\n",
    "pause = pd.read_csv(os.path.join(summary_dir, 'MeanUnvoicedSegmentLength_summary.csv'), sep = ';', index_col = [0])\n",
    "pitch_var = pd.read_csv(os.path.join(summary_dir, 'F0semitoneFrom27.5Hz_sma3nz_stddevNorm_summary.csv'), sep = ';', index_col = [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions to make looping easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndices(df, group):\n",
    "    \n",
    "    group_indices = [k for k in df['soundname'] if k[:4] in group]\n",
    "    \n",
    "    return group_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {}\n",
    "\n",
    "for file in glob.glob(dem_dir + os_sep + '*.txt'):\n",
    "    groupname = os.path.basename(file)[:-4]\n",
    "    \n",
    "    groups[groupname] = np.loadtxt(file, dtype= str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for t-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [('control_subs', 'patient_subs'), \n",
    "         ('high_panss_subs', 'low_panss_subs'),\n",
    "         ('control_subs', 'high_panss_subs'), \n",
    "         ('controls_m', 'controls_f'),\n",
    "         ('sz_m', 'sz_f'),\n",
    "         ('controls_same', 'controls_diff'),\n",
    "         ('sz_same', 'sz_diff')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['T', 'p']\n",
    "row_labels = ['f0', 'loudness', 'art_rate', 'avg_pause_dur', 'pitch_var']\n",
    "dfs = [pitch, loudness, syll, pause, pitch_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "test_dfs = {}\n",
    "\n",
    "for keys in tests:\n",
    "    \n",
    "    group1 = groups[keys[0]]\n",
    "    group2 = groups[keys[1]]\n",
    "    \n",
    "    rows = {}\n",
    "    \n",
    "    for row_label, df in zip(row_labels, dfs):\n",
    "            row = {}\n",
    "        \n",
    "            idxs_g1 = getIndices(df, group1) #the matching subjects in the dataframe\n",
    "            idxs_g2 = getIndices(df, group2)\n",
    "\n",
    "            values_g1 = df[df['soundname'].isin(idxs_g1)]['r_z']   #select converted r value\n",
    "            values_g2 = df[df['soundname'].isin(idxs_g2)]['r_z']          \n",
    "                            \n",
    "            t, p = stats.ttest_ind(values_g1, values_g2, equal_var = False) #equal var = False --> Welch's t-test\n",
    "            \n",
    "            row['T'] = t\n",
    "            row['p'] = p\n",
    "            \n",
    "            rows[row_label] = row\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    test_dfs[keys[0] + ' ' + keys[1]] = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_df = pd.concat(test_dfs.values(), keys = test_dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_df.to_csv(os.path.join(dem_dir, 'ttest_groups.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test against 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "one_test_dfs = {}\n",
    "ttest_groups = ['control_subs', 'patient_subs']\n",
    "\n",
    "for group in ttest_groups:\n",
    "    \n",
    "    rows = {}\n",
    "    group_subs = groups[group]\n",
    "\n",
    "    for row_label, df in zip(row_labels, dfs):\n",
    "        row = {}\n",
    "        \n",
    "        idxs = getIndices(df, group_subs) #the matching subjects in the dataframe\n",
    "        \n",
    "        values = df[df['soundname'].isin(idxs)]['r_z']   #select converted r value\n",
    "\n",
    "        t, p = stats.ttest_1samp(values, 0.0)\n",
    "        \n",
    "        row['T'] = t\n",
    "        row['p'] = p\n",
    "        \n",
    "        rows[row_label] = row\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    one_test_dfs[group] = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_test_df = pd.concat(one_test_dfs.values(), keys = one_test_dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.10f' % x)\n",
    "\n",
    "one_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_test_df.to_csv(os.path.join(dem_dir, 'one_sided_groups.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
