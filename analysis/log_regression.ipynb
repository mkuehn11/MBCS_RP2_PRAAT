{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm \n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_data = pd.read_csv('dem_data.csv') #file with symptom scores, gender, medication information etc\n",
    "\n",
    "# group information containing subject ids\n",
    "controls = np.loadtxt('control_subs.txt', dtype= str)\n",
    "patients = np.loadtxt('patient_subs.txt', dtype= str)\n",
    "all_subs = np.concatenate([controls, patients])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_sep = os.path.abspath(os.sep)\n",
    "wd = os.getcwd()\n",
    "dfs = os.path.join(os_sep, wd, 'opensmile', 'egemaps_summary_turns_zero_filtered') #feature dataframes\n",
    "summary_dir = os.path.join(os_sep, wd, 'group_level') #synchrony dataframes for all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load synchrony dataframes\n",
    "pitch = pd.read_csv(os.path.join(summary_dir, 'F0semitoneFrom27.5Hz_sma3nz_amean_summary.csv'), sep = ';', index_col = [0])\n",
    "loudness = pd.read_csv(os.path.join(summary_dir, 'loudness_sma3_amean_summary.csv'), sep = ';', index_col = [0])\n",
    "syll = pd.read_csv(os.path.join(summary_dir, 'VoicedSegmentsPerSec_summary.csv'), sep = ';', index_col = [0])\n",
    "pause = pd.read_csv(os.path.join(summary_dir, 'MeanUnvoicedSegmentLength_summary.csv'), sep = ';', index_col = [0])\n",
    "pitch_var = pd.read_csv(os.path.join(summary_dir, 'F0semitoneFrom27.5Hz_sma3nz_stddevNorm_summary.csv'), sep = ';', index_col = [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add accommodation coefficients as predictors for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataframe containing the predictors for all acoustic features\n",
    "\n",
    "feature_df = pd.DataFrame([pitch['r'], loudness['r'], syll['r'], pause['r'], pitch_var['r']]).T\n",
    "feature_df.columns = ['pitch', 'loudness', 'syll_rate', 'pause_dur', 'pitch_var']\n",
    "feature_df.index = pitch['soundname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interviewers = dem_data.loc[all_subs]['Interviewer'] #who conducted the interview\n",
    "yoe = dem_data.loc[all_subs]['YOE_handmatig'].sort_index() #years of education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#encode interviewers numerically\n",
    "le = LabelEncoder()\n",
    "interviewers_encod = le.fit_transform(interviewers)\n",
    "interviewers_encod[interviewers.isna()] =  -99999 #preserve invalid entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add YOE as predictor\n",
    "\n",
    "The interviewer can be added as a predictor as well, but this information is not available for all subjects so it comes with a great loss of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_df['yoe'] = yoe\n",
    "\n",
    "## optionally add interviewer as predictor\n",
    "#feature_df['interviewer'] = interviewers_encod\n",
    "#feature_df.replace(-99999, np.nan, inplace= True)\n",
    "#feature_df = feature_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print predictors\n",
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode target variable \"group\" numerically\n",
    "group_encoding = []\n",
    "\n",
    "for sub in feature_df.index:\n",
    "    if sub in controls:\n",
    "        group_encoding.append(0)\n",
    "    else:\n",
    "        group_encoding.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df = pd.DataFrame(group_encoding, columns = ['group'])\n",
    "group_df.index = feature_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assert indipendence of predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "\n",
    "X = feature_df  # independent variables\n",
    "y = group_df   # dependent variables\n",
    "X_intcpt = sm.add_constant(X)\n",
    "\n",
    "# Variance Inflation Factor (VIF) to analyze multicolinearity\n",
    "# Between 1 and 5 = moderately correlated.\n",
    "pd.DataFrame({'variables':X_intcpt.columns[1:], 'VIF':[variance_inflation_factor(X_intcpt.values, i+1) for i in range(len(X_intcpt.columns[1:]))]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data in train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (confusion_matrix, roc_auc_score) \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Recursive feature elimination to determine the optimal amount of features for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to select 1 variable: can be changed and checked in model for accuracy\n",
    "min_features_to_select = 1\n",
    "\n",
    "rfe_mod =  RFECV(estimator = LogisticRegression(), step=1, scoring = 'roc_auc')\n",
    "\n",
    "rfe_fit = rfe_mod.fit(X_train, np.array(y_train).flatten())\n",
    "\n",
    "# (i.e., estimated best) features are assigned rank 1.\n",
    "print(rfe_fit.ranking_)\n",
    "\n",
    "rfe_features = X.columns[rfe_fit.support_]\n",
    "print(rfe_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"AUC score\")\n",
    "plt.plot(range(min_features_to_select,\n",
    "               len(rfe_fit.grid_scores_) + min_features_to_select),\n",
    "         rfe_fit.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model using stratified 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "X_arr = X.to_numpy()\n",
    "y_arr = y.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_sklearn = LogisticRegression()\n",
    "\n",
    "tprs = [] #true positive rate\n",
    "aucs = [] #auc scores for each fold\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "i = 1\n",
    "\n",
    "for train, test in cv.split(X_arr, y_arr):\n",
    "    \n",
    "    log_fit = log_sklearn.fit(X_arr[train], y_arr[train])\n",
    "    \n",
    "    yhat_proba = log_fit.predict_proba(X_arr[test])\n",
    "    \n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_arr[test], yhat_proba[:, 1])\n",
    "    \n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    \n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC',fontsize=18)\n",
    "plt.legend(loc=\"lower right\", prop={'size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fit = log_sklearn.fit(X_train, np.array(y_train).flatten())\n",
    "\n",
    "yhat = log_sklearn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test['group'], yhat)\n",
    "\n",
    "fig, ax = plt.subplots(figsize= (7, 7))\n",
    "\n",
    "sns.heatmap(cm, annot=True, cmap = 'Blues', ax = ax)\n",
    "\n",
    "ax.set_yticklabels(['Control', 'Patient'], fontsize = 13)\n",
    "ax.set_xticklabels(['Control', 'Patient'], fontsize = 13)\n",
    "ax.set_title('Confusion Matrix', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_vs_prediction = y_test\n",
    "real_vs_prediction['yhat'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify which patients were correctly or incorrectly classified\n",
    "incorrect_ident = real_vs_prediction.loc[(real_vs_prediction['group'] == 1) & (real_vs_prediction['yhat'] == 0)]\n",
    "correct_ident = real_vs_prediction.loc[(real_vs_prediction['group'] == 1) & (real_vs_prediction['yhat'] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare the panss scores between the two groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panss_cols = ['PANSS_datum','PANSS_P1','PANSS_P2','PANSS_P3','PANSS_P4',\n",
    "              'PANSS_P5','PANSS_P6','PANSS_P7','PANSS_N1','PANSS_N2','PANSS_N3',\n",
    "              'PANSS_N4','PANSS_N5','PANSS_N6','PANSS_N7','PANSS_G1','PANSS_G2',\n",
    "              'PANSS_G3','PANSS_G4','PANSS_G5','PANSS_G6','PANSS_G7','PANSS_G8',\n",
    "              'PANSS_G9','PANSS_G10','PANSS_G11','PANSS_G12','PANSS_G13','PANSS_G14',\n",
    "              'PANSS_G15','PANSS_G16','PANSS_remission','PANSS_totaal','PANSS_positive',\n",
    "              'PANSS_negative','PANSS_general','PANSS_Positive_factor','PANSS_negative_factor',\n",
    "              'PANSS_disorganized_factor','PANSS_excited_factor','PANSS_depressed_factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panss_incorrect = dem_data.loc[incorrect_ident.index][panss_cols]\n",
    "panss_correct = dem_data.loc[correct_ident.index][panss_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out the participants with missing scores\n",
    "panss_correct_filt = panss_correct.loc[(panss_correct['PANSS_remission'] != 'geen PANSS') & (panss_correct['PANSS_totaal'] < 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panss_incorrect_filt = panss_incorrect.loc[(panss_incorrect['PANSS_remission'] != 'geen PANSS') & (panss_incorrect['PANSS_totaal'] < 1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print scores of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorr_neg = panss_incorrect_filt[['PANSS_N1','PANSS_N2','PANSS_N3','PANSS_N4','PANSS_N5','PANSS_N6','PANSS_N7']]\n",
    "incorr_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_neg = panss_correct_filt[['PANSS_N1','PANSS_N2','PANSS_N3','PANSS_N4','PANSS_N5','PANSS_N6','PANSS_N7']]\n",
    "corr_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the data for easier plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_neg_melt = corr_neg.melt(var_name='symptom', value_name='score')\n",
    "corr_neg_melt['soundname'] = np.concatenate([[corr_neg.index]] * 7).flatten()\n",
    "corr_neg_melt['kind'] = 'correctly_identified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorr_neg_melt = incorr_neg.melt(var_name='symptom', value_name='score')\n",
    "incorr_neg_melt['soundname'] = np.concatenate([[incorr_neg.index]] * 7).flatten()\n",
    "incorr_neg_melt['kind'] = 'incorrectly_identified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both = pd.concat([corr_neg_melt, incorr_neg_melt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(figsize= (13, 7))\n",
    "\n",
    "#plot scores of correctly identified patients\n",
    "sns.stripplot(x=\"symptom\", y=\"score\", data = corr_neg_melt, \n",
    "              marker = '^', size = 7, linewidth=0.5, color = '#91bfdb',  ax = ax)\n",
    "\n",
    "#plot scores of incorrectly identified patients\n",
    "sns.stripplot(x=\"symptom\", y=\"score\", data = incorr_neg_melt, \n",
    "              marker = 'D', size = 7, linewidth=0.5, color = '#fc8d59',  ax = ax)\n",
    "\n",
    "sns.pointplot(x=\"symptom\", y=\"score\", ci= None, capsize=.2, hue = 'kind', data = both)\n",
    "\n",
    "ax.set_ylabel('PANSS Score', size = 13)\n",
    "ax.set_xlabel('')\n",
    "\n",
    "ax.set_xticklabels(['Blunted Affect', 'Emotional Withdrawal', \n",
    "                    'Poor Rapport', 'Social Withdrawal', 'Difficulty Abstract Thinking',\n",
    "                    'Flow of conversation', 'Stereotyped Thinking'], rotation = 45, ha='right', fontsize = 13)\n",
    "\n",
    "\n",
    "corr_patch = mpatches.Patch(color = '#91bfdb', label='Correctly Identified Patients')\n",
    "incorr_patch = mpatches.Patch(color = '#fc8d59', label='Incorrectly Identified Patients')\n",
    "\n",
    "plt.legend(handles=[corr_patch, incorr_patch], fontsize = 13)\n",
    "plt.title('PANSS Scores of Negative Symptoms', fontsize = 15)\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
